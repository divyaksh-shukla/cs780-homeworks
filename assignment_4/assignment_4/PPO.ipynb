{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All imports here\n",
    "## Feel free to add or remove\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "gym_id = \"CartPole-v1\"  #The id of the gym environment\n",
    "learning_rate = 0.0001\n",
    "seed = 1\n",
    "total_timesteps =  10000#The total timesteps of the experiments\n",
    "torch_deterministic = True   #If toggled, `torch.backends.cudnn.deterministic=False\n",
    "cuda = True\n",
    "\n",
    "num_envs = 4  #The number of parallel game environments (Yes PPO works with vectorized environments)\n",
    "num_steps = 128 #The number of steps to run in each environment per policy rollout\n",
    "anneal_lr = True #Toggle learning rate annealing for policy and value networks\n",
    "gae = True #Use GAE for advantage computation\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95 #The lambda for the general advantage estimation\n",
    "num_minibatches = 4\n",
    "update_epochs =  10#The K epochs to update the policy\n",
    "norm_adv = True  #Toggles advantages normalization\n",
    "clip_coef = 0.2 #The surrogate clipping coefficient (See what is recommended in the paper!)\n",
    "clip_vloss = True #Toggles whether or not to use a clipped loss for the value function, as per the paper\n",
    "ent_coef =  0.001#Coefficient of the entropy\n",
    "vf_coef =  0.1#Coefficient of the value function\n",
    "max_grad_norm = 1\n",
    "target_kl = None #The target KL divergence threshold\n",
    "\n",
    "\n",
    "batch_size = int(num_envs * num_steps)\n",
    "minibatch_size = int(batch_size // num_minibatches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO works with vectorized enviromnets lets make a function that returns a function that returns an environment.\n",
    "#Refer how to make vectorized environments in gymnasium\n",
    "def make_env(gym_id, seed):\n",
    "    #Your code here\n",
    "    env = gym.make_vec(gym_id, num_envs=num_envs)\n",
    "    _ = env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_init(tensor, gain=1):\n",
    "    assert tensor.ndimension() >= 2\n",
    "    rows = tensor.size(0)\n",
    "    cols = tensor[0].numel()\n",
    "    flattened = tensor.new(rows, cols).normal_(0, 1)\n",
    "    if rows < cols:\n",
    "        flattened.t_()\n",
    "    u, s, v = torch.svd(flattened, some=True)\n",
    "    if rows < cols:\n",
    "        u.t_()\n",
    "    q = u if tuple(u.shape) == (rows, cols) else v\n",
    "    with torch.no_grad():\n",
    "        tensor.view_as(q).copy_(q)\n",
    "        tensor.mul_(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We initialize the layers in PPO , refer paper.\n",
    "#Lets initialize the layers with this function\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    if hasattr(layer, 'bias'):\n",
    "        layer.bias.data = layer.bias.data.zero_() + bias_const\n",
    "        orthogonal_init(layer.weight.data, gain=std)\n",
    "    #inplace initialization no need to return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_indices(dones):\n",
    "    indices = []\n",
    "    num_timesteps = dones.shape[1]\n",
    "    for actor in range(dones.shape[0]):\n",
    "        last_index = 0\n",
    "        for i in range(num_timesteps):\n",
    "            if dones[actor, i] == 0.:\n",
    "                indices.append((actor, last_index, i + 1))\n",
    "                last_index = i + 1\n",
    "        if last_index != num_timesteps:\n",
    "            indices.append((actor, last_index, num_timesteps))\n",
    "    return indices\n",
    "\n",
    "\n",
    "def discount_path(path, h):\n",
    "    curr = 0\n",
    "    rets = []\n",
    "    for i in range(len(path)):\n",
    "        curr = curr*h + path[-1-i]\n",
    "        rets.append(curr)\n",
    "    rets =  torch.stack(list(reversed(rets)), 0)\n",
    "    return rets\n",
    "\n",
    "\n",
    "def compute_advantage_return(rewards, values, dones, gamma=gamma, gae_lambda=gae_lambda):\n",
    "    rewards = rewards.T\n",
    "    values = values.T\n",
    "    dones = dones.T\n",
    "    V_s = torch.cat([values[:,1:], values[:, -1:]], 1) * dones\n",
    "    deltas = rewards + gamma * V_s - values\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    indices = get_path_indices(dones)\n",
    "    for agent, start, end in indices:\n",
    "        advantages[agent, start:end] = discount_path(deltas[agent, start:end], gae_lambda*gamma)\n",
    "        returns[agent, start:end] = discount_path(rewards[agent, start:end], gamma)\n",
    "    return advantages.T.clone().detach(), returns.T.clone().detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self, inDim, outDim, hDim, act='tanh'):\n",
    "            super().__init__()\n",
    "            act_fn = nn.Tanh() if act=='tanh' else nn.ReLU() \n",
    "            net_list = [nn.Linear(inDim, hDim[0]), act_fn]\n",
    "            for i in range(len(hDim)-1):\n",
    "                net_list += [nn.Linear(hDim[i], hDim[i+1]), act_fn]\n",
    "            net_list += [nn.Linear(hDim[-1], outDim)]\n",
    "            self.net = nn.Sequential(*net_list)\n",
    "            self.weight_init()\n",
    "            \n",
    "        def weight_init(self):\n",
    "            for i in range(len(self.net)):\n",
    "                layer_init(self.net[i])\n",
    "                \n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make the Main agent class\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        self.critic = Net(inDim=envs.observation_space.shape[-1],\n",
    "                              outDim=1, hDim=[64,64], act='tahn')\n",
    "                              \n",
    "        #(Returns a single value of the observation)\n",
    "        \n",
    "        self.actor = Net(inDim=envs.observation_space.shape[-1],\n",
    "                              outDim=envs.single_action_space.n, hDim=[64,64], act='tahn')\n",
    "        \n",
    "        #(Returns the logits of the actions on the observations)\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        # Returns the value from the critic on the observation x\n",
    "        return self.critic(x)\n",
    "        \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        #Returns 1.the action (sampled according to the logits), \n",
    "        #2.log_prob of the action,\n",
    "        #3.Entropy,\n",
    "        #4.Value from the critic\n",
    "        \n",
    "        #Your code here\n",
    "        logits = self.actor(x)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        actions = dist.sample()\n",
    "        logPs = dist.log_prob(actions)\n",
    "        ent = dist.entropy()\n",
    "        return actions, logPs, ent, self.get_value(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the vectorized environments, use the helper function that we have declared above\n",
    "envs = make_env('CartPole-v1', seed=42)# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(envs=envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=1e-5) #eps is not the default that pytorch uses\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((num_steps, num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, info = envs.reset()\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(num_envs).to(device)\n",
    "num_updates = total_timesteps // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
    "                                            lr_lambda=lambda f: 1-f/num_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:04<00:00,  4.21it/s]\n"
     ]
    }
   ],
   "source": [
    "#This is the main training loop where we collect the experience , \n",
    "#calculate the advantages, ratio , the total loss and learn the policy\n",
    "\n",
    "for update in tqdm(range(1, num_updates + 1)):\n",
    "    \n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if anneal_lr:\n",
    "        # Your code here \n",
    "        if update>1:\n",
    "            scheduler.step()\n",
    "        \n",
    "\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs  # We are taking a step in each environment \n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            #Get the action , logprob , _ , value from the agent.\n",
    "            \n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, done,truncated, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "        \n",
    "        for item in info.keys():\n",
    "            if item == \"final_info\" and info[item][0]:\n",
    "                print(f\"global_step={global_step}, episodic_return={info[item][0]['episode']['r']}\")\n",
    "                break\n",
    "    #import sys; sys.exit(0)\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        if gae:\n",
    "            advantages, returns = compute_advantage_return(rewards, values, dones)\n",
    "            #returns = advantages + values  (yes official implementation of ppo calculates it this way)\n",
    "        else:\n",
    "            # Your code here \n",
    "            #advantages = returns - values\n",
    "            pass\n",
    "            \n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(update_epochs):\n",
    "        #Get a random sample of batch_size\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            #Your code here\n",
    "            #Calculate the ratio\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds])\n",
    "            logratio =  b_logprobs[mb_inds] - newlogprob\n",
    "            ratio = torch.exp(logratio)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                # Refer the blog for calculating kl in a simpler way\n",
    "                #old_approx_kl = \n",
    "                #approx_kl = \n",
    "                clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss (Calculate the policy loss pg_loss)\n",
    "            # Your code here \n",
    "            pg_loss = -torch.min(mb_advantages*ratio, mb_advantages*torch.clamp(ratio, 1-clip_coef, 1+clip_coef)).mean()\n",
    "            # Value loss v_loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            vs_tgt = (b_values[mb_inds] + mb_advantages).detach()\n",
    "            v_clipped = b_values[mb_inds] + torch.clamp(newvalue - b_values[mb_inds],-clip_coef,clip_coef )\n",
    "            sel = dones.view(-1)[mb_inds].bool()\n",
    "            val_loss_unclipped = (newvalue - vs_tgt)[sel].pow(2)\n",
    "            val_loss_clipped = (v_clipped - vs_tgt)[sel].pow(2)\n",
    "            if clip_vloss:\n",
    "                v_loss = torch.max(val_loss_unclipped,val_loss_clipped).mean()\n",
    "            else:\n",
    "                v_loss = val_loss_unclipped.mean()\n",
    "\n",
    "            # Entropy loss \n",
    "            entropy_loss = entropy.mean()\n",
    "\n",
    "            # Total loss\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        # if target_kl is not None:\n",
    "        #     if approx_kl > target_kl:\n",
    "        #         break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
